{
    "collab_server" : "",
    "contents" : "# clean working directory\nrm(list = ls(all = TRUE))\n\n# Set working directory\nsetwd(\"~/Dropbox/shared/experimento\")\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\n\nept_df1 <- read_csv(\"./data/ept_raw.csv\") %>% \n           mutate(., groupCon = recode(group, co = 0.5, hs = -0.5), \n                     freqCon = recode(frequency, f = 0.5, u = -0.5))\n\nept_df2 <- read_csv(\"./data/ept_additional_data.csv\") %>% \n           mutate(., groupCon = recode(group, co = 0.5, hs = -0.5), \n                     freqCon = recode(frequency, f = 0.5, u = -0.5))\n\n\n\n\nept_df_temp <- rbind(ept_df1, ept_df2)\n\n\nverbs <- c('f1' = 'cuidar', \n           'f2' = 'encontrar', \n           'f3' = 'tocar', \n           'f4' = 'aceptar', \n           'f5' = 'medir', \n           'f6' = 'parar', \n           'f7' = 'llevar', \n           'f8' = 'cambiar',\n           'u1' = 'vigilar',\n           'u2' = 'hallar', \n           'u3' = 'acariciar', \n           'u4' = 'acoger', \n           'u5' = 'abrigar', \n           'u6' = 'detener', \n           'u7' = 'trasladar', \n           'u8' = 'reemplazar')\n\nept_df_temp$verbs <- verbs[ept_df_temp$item]\n\n\nunique(ept_df_temp$participant)\n\n\n\n\n\n# import dele data and join with df\nept_df <- read_csv(\"./data/prof_df.csv\") %>% \n          left_join(ept_df_temp, ., by = 'participant')\n\n\n\nglimpse(ept_df)\n\n\n# !!!! NO MORE OPENING THE FOLLOWING FILES:\n# - ajt_comp_clean.csv\n# - ajt_raw.csv\n# - ept_comp_clean.csv\n# - ept_raw.csv\n\n# proficiency\n# - create data frame (excel) with participant ID and prof label and dele (participant, prof, dele)\n# - save as 'prof_df.csv' in data folder\n\n# frequency\n# - create data frame (excel) with verb (infinitive) and frequency (davis score)\n# - 2 columns, verb and freq\n# - save as 'freq_df.csv' in data folder\n\n# additional participants\n# - create new data frame for additional participants\n# - same columns as 'ajt_raw.csv' and 'ept_raw.csv'\n# - save as 'ajt_additional_data.csv' in data folder\n# - save as 'ept_additional_data.csv' in data folder\n\n\n\n\n\n\n\n\n\n\n\n\n# response as a function of group and freq\n# - calculate means\n# - make a graph\n\nept_df %>% \n  group_by(., group, proficiency, frequency) %>% \n  summarize(., mean = mean(response), \n               sd = sd(response))\n\n# group proficiency frequency      mean         sd\n#    co          co         f 0.9916667 0.09128709\n#    co          co         u 1.0000000 0.00000000\n#    hs      hs_adv         f 0.4625000 0.50015721\n#    hs      hs_adv         u 0.4187500 0.49490327\n#    hs      hs_int         f 0.2125000 0.41165766\n#    hs      hs_int         u 0.0750000 0.26505314\n\nept_df %>% \n  group_by(., group, proficiency) %>% \n  summarize(., mean = mean(response), \n               sd = sd(response))\n\n# group proficiency      mean         sd\n#    co          co 0.9958333 0.06454972\n#    hs      hs_adv 0.4406250 0.49723964\n#    hs      hs_int 0.1437500 0.35193758\n\n\nept_p1 <- ept_df %>% \n  ggplot(., aes(x = proficiency, y = response, color = frequency, dodge = frequency)) + \n    stat_summary(fun.data = mean_cl_boot, geom = 'pointrange', position = position_dodge(1), size = 0.75) + \n    stat_summary(fun.y = mean, geom = 'point', position = position_dodge(1), color = 'white', size = 2) + \n    scale_color_brewer(palette = \"Set1\", name = \"\", labels = c(\"Frequent\", 'Infrequent')) + \n    scale_x_discrete(labels = c(\"CO\", \"ADV\", \"INT\")) + \n    ylim(0, 1) + \n    labs(y = '% DOM', x = 'Group', caption = \"Mean +/- 95% CI\") + \n    theme_grey(base_family = 'Times', base_size = 12)\n\n# ggsave(filename = 'ept_p1.png', plot = ept_p1, path = \"./figs\", \n#        width = 5, height = 4, units = \"in\", dpi = 300)\n\n# response as a fucntion of item and freq\n# relevel verbs according to \n\norder <- ept_df %>% \n  group_by(., verbs, frequency, group) %>% \n  summarize(., prop = mean(response)) %>% \n  arrange(group, frequency, desc(prop)) %>% \n  as.data.frame %>% \n  slice(17:32) %>% \n  pull(verbs)\n\nept_df$verbs <- factor(ept_df$verbs, levels = order)\n\nept_p2 <- ept_df %>% \n  # filter(., !(participant %in% c(\"p03\", \"p13\"))) %>%\n  ggplot(., aes(x = verbs, y = response, color = frequency, dodge = frequency)) + \n    facet_grid(proficiency ~ .) + \n    stat_summary(fun.data = mean_cl_boot, geom = 'pointrange', position = position_dodge(1)) + \n    stat_summary(fun.y = mean, geom = 'point', position = position_dodge(1), color = 'white', size = 2) + \n    scale_color_brewer(palette = \"Set1\", name = \"\", labels = c(\"Frequent\", 'Infrequent')) + \n    labs(y = '% DOM', x = 'Verb', caption = \"Mean +/- 95% CI\") + \n    theme_grey(base_family = 'Times', base_size = 12) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n# ggsave(filename = 'ept_p2.png', plot = ept_p2, path = \"./figs\", \n#        width = 5, height = 7, units = \"in\", dpi = 300)\n\n\n\n\n\n\n# logistic regression\n# Generalized linear mixed effects model\n# w/ binomial linking function\n# Random intercepts for subjects and item\n# Random slope for frequency\n# group and freq are deviation coded so \n# model output gives main effects\n\n\nept_elog <- ept_df %>% \n  group_by(., participant, proficiency, frequency, groupCon, freqCon) %>% \n  summarize(., n = 8, \n              wDOM = sum(response), \n              woDOM = n - wDOM, \n              eLog = log((wDOM + 0.5) / (n - wDOM + 0.5)), \n              wts = 1 / (wDOM + 0.5) + 1 / (n - wDOM + 0.5))\n\n\nlm_null <- lmer(eLog ~ 1 + \n               (1 | participant),\n                  data = ept_elog, weights = 1/wts, \n                  control=lmerControl(optimizer=\"bobyqa\"))\nlm_freq <- lmer(eLog ~ freqCon + \n               (1 | participant),\n                  data = ept_elog, weights = 1/wts, \n                  control=lmerControl(optimizer=\"bobyqa\"))\n\nlm_prof <- lmer(eLog ~ freqCon + proficiency + \n               (1 | participant),\n                  data = ept_elog, weights = 1/wts, \n                  control=lmerControl(optimizer=\"bobyqa\"))\n\nlm_full <- lmer(eLog ~ freqCon * proficiency + \n               (1 | participant),\n                  data = ept_elog, weights = 1/wts, \n                  control=lmerControl(optimizer=\"bobyqa\"))\n\n\nanova(lm_null, lm_freq, lm_prof, lm_full, test = 'Chisq')\n\nsummary(lm_full)\nranef(lm_full)\n\n# MuMIn::r.squaredGLMM(lm_full)\n#  R2m       R2c \n# 0.49      0.96\n\n# Main effect of group: X2(2) = 45.61, p < 0.001\n# No main effect of frequency: t = -0.56, p > 0.05\n# group by frequency interaction: X2(2) = 14.77, p < 0.001\n\n\n\n# We know the HS are different from controls, but are they \n# different from each other?\n# Fit another model without controls\nept_elog_hs <- ept_elog %>% filter(., proficiency != 'co') %>% \n  mutate(., profCon = if_else(proficiency == 'hs_adv', true = 0.5, false = -0.5))\n\nlm_hs <- lmer(eLog ~ freqCon * profCon + \n             (1 | participant), \n             data = ept_elog_hs, weights = 1/wts, \n             control = lmerControl(optimizer = 'bobyqa')) \nsummary(lm_hs)\n\n# Fixed effects:\n#                 Estimate Std. Error      df t value Pr(>|t|)    \n# (Intercept)      -1.1199     0.3252 27.0870  -3.444  0.00188 ** \n# freqCon           0.6051     0.1279 27.1390   4.732 6.19e-05 ***\n# profCon           1.3186     0.6504 27.0870   2.027  0.05256 .  \n# freqCon:profCon  -0.6792     0.2558 27.1390  -2.656  0.01309 *  \n\n# Main effect of freq  (t(27.14) = 4.73, p < 0.001) - More frequent verb had higher DOM\n# Main effect of group (t(27.09) = 2.03, p = 0.05)  - Adv HS used more DOM\n# Freq by group inter  (t(27.14) = -2.66, p < 0.02) - The effect of frequency was higher for int HS\n\n\n# group proficiency      mean         sd\n#    co          co 0.9958333 0.06454972\n#    hs      hs_adv 0.4406250 0.49723964\n#    hs      hs_int 0.1437500 0.35193758\n\n\n# group proficiency frequency      mean         sd\n#    co          co         f 0.9916667 0.09128709\n#    co          co         u 1.0000000 0.00000000\n#    hs      hs_adv         f 0.4625000 0.50015721\n#    hs      hs_adv         u 0.4187500 0.49490327\n#    hs      hs_int         f 0.2125000 0.41165766\n#    hs      hs_int         u 0.0750000 0.26505314\n\n\n\n\n\n\n\n\n\n# Compare AJT and EPT\n# - we will calculate a DOM score for frequent/unfrequent conditions \n#   for each participant in each group\n# - This will collapse over item\n# - each participant with have 2 scores\n\nept_elog %>% \n  group_by(., participant, proficiency, frequency) %>% \n  summarize(., avg_ept = mean(eLog)) %>% \n  write_csv(., \"./data/ept_comp_clean.csv\")",
    "created" : 1525810752782.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1174367189",
    "id" : "F66C9646",
    "lastKnownWriteTime" : 1513624752,
    "last_content_update" : 1513624752,
    "path" : "~/Dropbox/experimento/scripts/analysis_ept.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}